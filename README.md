# Attention-mechanism-implementation
In this project, we are calculating the attention values (from 0 to 1) by a weighted sum of the value vectors, in terms of weight factor for a group of four different words.
Attention is calculated as a product of weight factor for each vector and the values. So, the aim of implementation this mechanism was to The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights.
