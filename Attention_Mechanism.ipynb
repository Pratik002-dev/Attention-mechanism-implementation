{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu5seNiqQ4rs"
      },
      "outputs": [],
      "source": [
        "#letâ€™s start by first defining the word embeddings of the four different words for which we will be calculating the attention.\n",
        "from numpy import array\n",
        "from numpy import random\n",
        "from numpy import dot\n",
        "from scipy.special import softmax\n",
        "# encoder representations of four different words\n",
        "word_1 = array([1, 0, 0])\n",
        "word_2 = array([0, 1, 0])\n",
        "word_3 = array([1, 1, 0])\n",
        "word_4 = array([0, 0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the weight matrices which we will eventually be multiplying to the word embeddings to generate the queries, keys and values.\n",
        "random.seed(42) # to allow us to reproduce the same attention values\n",
        "W_Q = random.randint(3, size=(3, 3))\n",
        "W_K = random.randint(3, size=(3, 3))\n",
        "W_V = random.randint(3, size=(3, 3))"
      ],
      "metadata": {
        "id": "9SRUkGVARHQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the queries, keys and values by multiplying each word embedding by each of the weight matrices. \n",
        "query_1 = word_1 @ W_Q\n",
        "key_1 = word_1 @ W_K\n",
        "value_1 = word_1 @ W_V\n",
        " \n",
        "query_2 = word_2 @ W_Q\n",
        "key_2 = word_2 @ W_K\n",
        "value_2 = word_2 @ W_V\n",
        " \n",
        "query_3 = word_3 @ W_Q\n",
        "key_3 = word_3 @ W_K\n",
        "value_3 = word_3 @ W_V\n",
        " \n",
        "query_4 = word_4 @ W_Q\n",
        "key_4 = word_4 @ W_K\n",
        "value_4 = word_4 @ W_V"
      ],
      "metadata": {
        "id": "RjeEep_NRO8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scoring the first query vector against all key vectors\n",
        "scores = array([dot(query_1, key_1), dot(query_1, key_2), dot(query_1, key_3), dot(query_1, key_4)])"
      ],
      "metadata": {
        "id": "uEbCzZkTRPTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The score values are subsequently passed through a softmax operation to generate the weights.\n",
        "# computing the weights by a softmax operation\n",
        "weights = softmax(scores / key_1.shape[0] ** 0.5)"
      ],
      "metadata": {
        "id": "gA_4pPHkRTHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
        " \n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24vbViSTRTK3",
        "outputId": "c5627d0f-b629-4bab-b37a-66e0215ea4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.98522025 1.74174051 0.75652026]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy import array\n",
        "from numpy import random\n",
        "from numpy import dot\n",
        "from scipy.special import softmax\n",
        " \n",
        "# encoder representations of four different words\n",
        "word_1 = array([1, 0, 0])\n",
        "word_2 = array([0, 1, 0])\n",
        "word_3 = array([1, 1, 0])\n",
        "word_4 = array([0, 0, 1])\n",
        " \n",
        "# stacking the word embeddings into a single array\n",
        "words = array([word_1, word_2, word_3, word_4])\n",
        " \n",
        "# generating the weight matrices\n",
        "random.seed(42)\n",
        "W_Q = random.randint(3, size=(3, 3))\n",
        "W_K = random.randint(3, size=(3, 3))\n",
        "W_V = random.randint(3, size=(3, 3))\n",
        " \n",
        "# generating the queries, keys and values\n",
        "Q = words @ W_Q\n",
        "K = words @ W_K\n",
        "V = words @ W_V\n",
        " \n",
        "# scoring the query vectors against all key vectors\n",
        "scores = Q @ K.transpose()\n",
        " \n",
        "# computing the weights by a softmax operation\n",
        "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
        " \n",
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = weights @ V\n",
        " \n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbZHz6iLRZbB",
        "outputId": "cf55bdd8-1847-4f34-918b-7cb4f6c794e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.98522025 1.74174051 0.75652026]\n",
            " [0.90965265 1.40965265 0.5       ]\n",
            " [0.99851226 1.75849334 0.75998108]\n",
            " [0.99560386 1.90407309 0.90846923]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLrm4cdLRZhG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}